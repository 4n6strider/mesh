# -*-Python-*-

import mesh_tensorflow.transformer.transformer_layers

# for Unitransforemer models (e.g. language-model)
transformer.make_layer_stack.layers = [
    @mesh_tensorflow.transformer.transformer_layers.SelfAttention,
    @mesh_tensorflow.transformer.transformer_layers.DenseReluDense,
]

# for Bitransformer models (two-stack sequence-to-sequence models)
encoder/transformer.make_layer_stack.layers = [
    @mesh_tensorflow.transformer.transformer_layers.SelfAttention,
    @mesh_tensorflow.transformer.transformer_layers.DenseReluDense,
]
decoder/transformer.make_layer_stack.layers = [
    @mesh_tensorflow.transformer.transformer_layers.SelfAttention,
    @mesh_tensorflow.transformer.transformer_layers.EncDecAttention,
    @mesh_tensorflow.transformer.transformer_layers.DenseReluDense,
]

# dropout
dropout_rate = 0.1
DenseReluDense.dropout_rate = %dropout_rate
SelfAttention.dropout_rate = %dropout_rate
LocalSelfAttention.dropout_rate = %dropout_rate
LayerStack.dropout_rate = %dropout_rate

# label smoothing by default for Bitransofrmer models (but not language models)
decoder/Unitransformer.label_smoothing = 0.1

sequence_length = 256
# the sequence length for this run
utils.run.sequence_length = %sequence_length
# the maximum sequence length (changing breaks checkpoint compatibility)
Unitransformer.max_length = %sequence_length
